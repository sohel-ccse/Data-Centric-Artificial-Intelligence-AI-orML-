{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Source Code:https://www.kaggle.com/sohelranaccselab/car-price-prediction-using-advance-ai","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Enivornment Setup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Read, Data Visualization,EDA Analysis,Data Pre-Processing,Data Splitting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Read\nfile_path = '../input/car-price-prediction'\ndf=pd.read_csv(f'{file_path}/CarPrice_Assignment.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[:,~df.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing profile report\n\nprofile_report = pandas_profiling.ProfileReport(df,minimal=True)\nprofile_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.price.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.apply(lambda x: sum(x.isnull()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"CarName\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_matrix(d):\n    from matplotlib import pyplot as plt\n    from matplotlib import cm as cm\n\n    fig = plt.figure(figsize=(16,12))\n    ax1 = fig.add_subplot(111)\n    cmap = cm.get_cmap('jet', 30)\n    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n    ax1.grid(True)\n    plt.title('Car Prediction dataset features correlation\\n',fontsize=15)\n    labels=df.columns\n    ax1.set_xticklabels(labels,fontsize=9)\n    ax1.set_yticklabels(labels,fontsize=9)\n    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n    fig.colorbar(cax, ticks=[0.1*i for i in range(-11,11)])\n    plt.show()\n\ncorrelation_matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting data \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(df.corr(),annot=True,linecolor='red',linewidths=3,cmap = 'plasma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,diag_kind=\"kde\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = ['car_ID'] \ndf = df.drop(drop_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nfor col in df.select_dtypes('object').columns:\n    df[col] = le.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(df.corr(),annot=True,linecolor='red',linewidths=3,cmap = 'plasma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train = df[df['price'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test = df[df['price'].isnull()].drop(['price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_train.drop('price', axis=1)\ny = new_train['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the target variable countplot\nsns.countplot(data=new_train,x = 'price',palette='plasma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import  train_test_split, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distplot\nfig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(df['horsepower'],ax=ax2[0][0])\nsns.distplot(df['peakrpm'],ax=ax2[0][1])\nsns.distplot(df['citympg'],ax=ax2[0][2])\nsns.distplot(df['highwaympg'],ax=ax2[1][0])\nsns.distplot(df['compressionratio'],ax=ax2[1][1])\nsns.distplot(df['stroke'],ax=ax2[1][2])\nsns.distplot(df['boreratio'],ax=ax2[2][0])\nsns.distplot(df['boreratio'],ax=ax2[2][1])\nsns.distplot(df['fuelsystem'],ax=ax2[2][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nsc = RobustScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom scipy.stats import pearsonr\nwarnings.filterwarnings(\"ignore\")\n\ntarget = \"price\"\ndef model(algorithm,dtrainx,dtrainy,dtestx,dtesty,of_type,plot=False):\n    \n    print (algorithm)\n    print (\"***************************************************************************\")\n    algorithm.fit(dtrainx,dtrainy)\n    \n    #print(algorithm.get_params(deep=True))\n    \n    prediction = algorithm.predict(dtestx)\n    \n    print (\"ROOT MEAN SQUARED ERROR :\", np.sqrt(mean_squared_error(dtesty,prediction)) )\n    print (\"***************************************************************************\")\n    \n    print ('Performance on training data :', algorithm.score(dtrainx,dtrainy)*100)\n    print ('Performance on testing data :', algorithm.score(dtestx,dtesty)*100)\n\n    print (\"***************************************************************************\")\n    if plot==True:\n        sns.jointplot(x=dtesty, y=prediction, stat_func=pearsonr,kind=\"reg\", color=\"b\") \n    \n       \n    prediction = pd.DataFrame(prediction)\n    cross_val = cross_val_score(algorithm,dtrainx,dtrainy,cv=5)#,scoring=\"neg_mean_squared_error\"\n    cross_val = cross_val.ravel()\n    print (\"CROSS VALIDATION SCORE\")\n    print (\"************************\")\n    print (\"cv-mean :\",cross_val.mean()*100)\n    print (\"cv-std  :\",cross_val.std()*100)\n    \n    if plot==True:\n        plt.figure(figsize=(20,22))\n        plt.subplot(211)\n\n        testy = dtesty.reset_index()[\"price\"]\n\n        ax = testy.plot(label=\"originals\",figsize=(20,9),linewidth=2)\n        ax = prediction[0].plot(label = \"predictions\",figsize=(20,9),linewidth=2)\n        plt.legend(loc=\"best\")\n        plt.title(\"ORIGINALS VS PREDICTIONS\")\n        plt.xlabel(\"index\")\n        plt.ylabel(\"values\")\n        ax.set_facecolor(\"k\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nxgr =XGBRegressor(random_state=42)\nmodel(xgr,X_train,y_train,X_test,y_test,\"feat\",True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgr_1=XGBRegressor(random_state=42,learning_rate = 0.03,\n                max_depth = 9, n_estimators = 1000,n_jobs=-1,reg_alpha=0.005,gamma=0.1,subsample=0.7,colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.9)\nmodel(xgr_1,X_train,y_train,X_test,y_test,\"feat\",True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nparam_grid={'n_estimators' : [1000,2000,3000,2500],\n            'max_depth' : [1,2, 3,5,7,9,10,11,15],\n            'learning_rate' :[ 0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.8, 1.0],\n                                                     }\n# Create a base model\nxgbr = XGBRegressor(random_state = 42,reg_alpha=0.005,gamma=0.1,subsample=0.7,colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.9)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = xgbr, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\nmodel(best_grid,X_train,y_train,X_test,y_test,\"feat\",True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  RandomForestRegressor\nrf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=80,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\nmodel(rf,X_train,y_train,X_test,y_test,\"feat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_model = CatBoostRegressor(iterations=2000,\n                             learning_rate=0.03,\n                             depth=9,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(cb_model,X_train,y_train,X_test,y_test,\"feat\",True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multiple Machine Learning Algorithm for Resgression ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dict = {\n    'LinearRegession': LinearRegression(),\n    'Ridge':Ridge(),\n    'Lasso':Lasso(),\n    'KernelRidge':KernelRidge(),\n    'SGDRegressor':SGDRegressor(),\n    'BayesianRidge':BayesianRidge(),\n    'ElasticNet': ElasticNet(),\n    'LinearSVR':LinearSVR(),\n    #Perfect Models this Problem\n    'XGBRegressor':XGBRegressor(random_state=42, n_estimators=2000, max_depth=9),\n    'RandomForestRegressor': RandomForestRegressor(random_state=0, n_estimators=2000, max_depth=9),\n    'GradientBoostingRegressor': GradientBoostingRegressor(random_state=42, n_estimators=2000, max_depth=9, learning_rate=0.01)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list = list()\nfor name, model in model_dict.items():\n    data_dict = dict()\n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    data_dict['model'] = name\n    data_dict['train_score'] = train_score\n    data_dict['test_score'] = test_score\n    data_list.append(data_dict)\nscore_df = pd.DataFrame(data_list)\nscore_df['score_diff'] = score_df['train_score'] - score_df['test_score']\nmodel_df = score_df.sort_values(['test_score'], ascending=[False])\nmodel_df[model_df['test_score'] > 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\"X = new_train.drop('price', axis=1)\ny = new_train['price']\nfor ind, m_name in enumerate(model_df['model'].tolist()):\n    model = model_dict[m_name].fit(X, y)\n    predictions = model.predict(new_test)\n    test['price'] = predictions\n    test[['ID','price']].to_csv('Submission{}_{}.csv'.format(ind+1, m_name), index=False)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Artificial Neural Networks(ANNs) Part:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building ANN As a Regressor\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend\n\n#Defining Root Mean Square Error As our Metric Function \ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\n# Initialising the Artificial Neural Networks(ANNs)\nmodel_nn = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel_nn.add(Dense(512, activation = 'relu', input_dim = 24))\nmodel_nn.add(BatchNormalization())\n# Adding the second hidden layer\nmodel_nn.add(Dense(units = 256, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\n# Adding the third hidden layer\nmodel_nn.add(Dense(units = 256, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\nmodel_nn.add(Dense(units = 128, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\n# Adding the output layer\nmodel_nn.add(Dense(units = 1))\n\n# Optimize , Compile And Train The Model \nopt =keras.optimizers.Adam(lr=0.003)\n#print(model_nn.summary())\nmodel_nn.compile(optimizer=opt,loss='mean_squared_error',metrics=[rmse])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ncheckpoint_filepath ='best.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_rmse',\n    mode='min',\n    save_best_only=True)\n\n# Model weights are saved at the end of every epoch, if it's the best seen\n# so far.\nhistory=model_nn.fit(sc.fit_transform(X_train),y_train,epochs = 300 ,batch_size=32,validation_data=(sc.transform(X_test), y_test), callbacks=[model_checkpoint_callback])\n\n# The model weights (that are considered the best) are loaded into the model.\nmodel_nn.load_weights(checkpoint_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting and Finding R Squared Score\ny_predict = model_nn.predict(sc.transform(X_test))\nprint('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_test, y_predict))) \n\nplt.figure(figsize=(20,5))\nplt.plot(list(y_test) ,color = 'red', label = 'Real data',marker='o')\nplt.plot(y_predict, color = 'blue', label = 'Predicted data',marker='o')\nplt.title('Prediction')\nplt.legend()\nplt.show()\n\n# Plotting Loss And Root Mean Square Error For both Training And Test Sets\nplt.plot(history.history['rmse'])\nplt.plot(history.history['val_rmse'])\nplt.title('Root Mean Squared Error')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}